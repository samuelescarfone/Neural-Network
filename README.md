# Neural-Network

A hands‑on exploration of how feed‑forward neural networks really work, built **from scratch** with nothing but NumPy and Pandas, then benchmarked against a compact Keras baseline.


## Goals
• Understand forward pass, back‑propagation, and gradient descent by implementing each step manually.  
• Train the scratch network on the Kaggle **Digit Recognizer** (MNIST) dataset.  
• Build a comparable fully‑connected model in Keras and measure the gap in accuracy, speed, and learning dynamics.  
• Use insights from the Keras model (optimizers, hidden‑layer size, activation choices) to iteratively tune the manual network.

